{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dschreib/COGS185/SegNet_Tutorial/Scripts\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "\n",
    "currentDir = os.getcwd()\n",
    "print currentDir\n",
    "\n",
    "os.chdir('/home/dschreib/caffe_segnet')\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, './python')\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import caffe\n",
    "\n",
    "from pylab import *\n",
    "%matplotlib inline\n",
    "\n",
    "import scipy\n",
    "import argparse\n",
    "import math\n",
    "import cv2\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from skimage.io import ImageCollection\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import caffe\n",
    "from caffe.proto import caffe_pb2\n",
    "from google.protobuf import text_format\n",
    "\n",
    "os.chdir(currentDir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_dataset(net_message):\n",
    "    assert net_message.layer[0].type == \"DenseImageData\"\n",
    "    source = net_message.layer[0].dense_image_data_param.source\n",
    "    with open(source) as f:\n",
    "        data = f.read().split()\n",
    "    ims = ImageCollection(data[::2])\n",
    "    labs = ImageCollection(data[1::2])\n",
    "    assert len(ims) == len(labs) > 0\n",
    "    return ims, labs\n",
    "\n",
    "\n",
    "def make_testable(train_model_path):\n",
    "    # load the train net prototxt as a protobuf message\n",
    "    with open(train_model_path) as f:\n",
    "        train_str = f.read()\n",
    "    train_net = caffe_pb2.NetParameter()\n",
    "    text_format.Merge(train_str, train_net)\n",
    "\n",
    "    # add the mean, var top blobs to all BN layers\n",
    "    for layer in train_net.layer:\n",
    "        if layer.type == \"BN\" and len(layer.top) == 1:\n",
    "            layer.top.append(layer.top[0] + \"-mean\")\n",
    "            layer.top.append(layer.top[0] + \"-var\")\n",
    "\n",
    "    # remove the test data layer if present\n",
    "    if train_net.layer[1].name == \"data\" and train_net.layer[1].include:\n",
    "        train_net.layer.remove(train_net.layer[1])\n",
    "        if train_net.layer[0].include:\n",
    "            # remove the 'include {phase: TRAIN}' layer param\n",
    "            train_net.layer[0].include.remove(train_net.layer[0].include[0])\n",
    "    return train_net\n",
    "\n",
    "\n",
    "def make_test_files(testable_net_path, train_weights_path, num_iterations,\n",
    "                    in_h, in_w):\n",
    "    # load the train net prototxt as a protobuf message\n",
    "    with open(testable_net_path) as f:\n",
    "        testable_str = f.read()\n",
    "    testable_msg = caffe_pb2.NetParameter()\n",
    "    text_format.Merge(testable_str, testable_msg)\n",
    "    \n",
    "    bn_layers = [l.name for l in testable_msg.layer if l.type == \"BN\"]\n",
    "    bn_blobs = [l.top[0] for l in testable_msg.layer if l.type == \"BN\"]\n",
    "    bn_means = [l.top[1] for l in testable_msg.layer if l.type == \"BN\"]\n",
    "    bn_vars = [l.top[2] for l in testable_msg.layer if l.type == \"BN\"]\n",
    "\n",
    "    net = caffe.Net(testable_net_path, train_weights_path, caffe.TEST)\n",
    "    \n",
    "    # init our blob stores with the first forward pass\n",
    "    res = net.forward()\n",
    "    bn_avg_mean = {bn_mean: np.squeeze(res[bn_mean]).copy() for bn_mean in bn_means}\n",
    "    bn_avg_var = {bn_var: np.squeeze(res[bn_var]).copy() for bn_var in bn_vars}\n",
    "\n",
    "    # iterate over the rest of the training set\n",
    "    for i in xrange(1, num_iterations):\n",
    "        res = net.forward()\n",
    "        for bn_mean in bn_means:\n",
    "            bn_avg_mean[bn_mean] += np.squeeze(res[bn_mean])\n",
    "        for bn_var in bn_vars:\n",
    "            bn_avg_var[bn_var] += np.squeeze(res[bn_var])\n",
    "        print 'progress: {}/{}'.format(i, num_iterations)\n",
    "\n",
    "    # compute average means and vars\n",
    "    for bn_mean in bn_means:\n",
    "        bn_avg_mean[bn_mean] /= num_iterations\n",
    "    for bn_var in bn_vars:\n",
    "        bn_avg_var[bn_var] /= num_iterations\n",
    "\n",
    "    for bn_blob, bn_var in zip(bn_blobs, bn_vars):\n",
    "        m = np.prod(net.blobs[bn_blob].data.shape) / np.prod(bn_avg_var[bn_var].shape)\n",
    "        bn_avg_var[bn_var] *= (m / (m - 1))\n",
    "\n",
    "    # calculate the new scale and shift blobs for all the BN layers\n",
    "    scale_data = {bn_layer: np.squeeze(net.params[bn_layer][0].data)\n",
    "                  for bn_layer in bn_layers}\n",
    "    shift_data = {bn_layer: np.squeeze(net.params[bn_layer][1].data)\n",
    "                  for bn_layer in bn_layers}\n",
    "\n",
    "    var_eps = 1e-9\n",
    "    new_scale_data = {}\n",
    "    new_shift_data = {}\n",
    "    for bn_layer, bn_mean, bn_var in zip(bn_layers, bn_means, bn_vars):\n",
    "        gamma = scale_data[bn_layer]\n",
    "        beta = shift_data[bn_layer]\n",
    "        Ex = bn_avg_mean[bn_mean]\n",
    "        Varx = bn_avg_var[bn_var]\n",
    "        new_gamma = gamma / np.sqrt(Varx + var_eps)\n",
    "        new_beta = beta - (gamma * Ex / np.sqrt(Varx + var_eps))\n",
    "\n",
    "        new_scale_data[bn_layer] = new_gamma\n",
    "        new_shift_data[bn_layer] = new_beta\n",
    "    print \"New data:\"\n",
    "    print new_scale_data.keys()\n",
    "    print new_shift_data.keys()\n",
    "\n",
    "    # assign computed new scale and shift values to net.params\n",
    "    for bn_layer in bn_layers:\n",
    "        net.params[bn_layer][0].data[...] = new_scale_data[bn_layer].reshape(\n",
    "            net.params[bn_layer][0].data.shape\n",
    "        )\n",
    "        net.params[bn_layer][1].data[...] = new_shift_data[bn_layer].reshape(\n",
    "            net.params[bn_layer][1].data.shape\n",
    "        )\n",
    "        \n",
    "    # build a test net prototxt\n",
    "    test_msg = testable_msg\n",
    "    # replace data layers with 'input' net param\n",
    "    data_layers = [l for l in test_msg.layer if l.type.endswith(\"Data\")]\n",
    "    for data_layer in data_layers:\n",
    "        test_msg.layer.remove(data_layer)\n",
    "    test_msg.input.append(\"data\")\n",
    "    test_msg.input_dim.append(1)\n",
    "    test_msg.input_dim.append(3)\n",
    "    test_msg.input_dim.append(in_h)\n",
    "    test_msg.input_dim.append(in_w)\n",
    "    # Set BN layers to INFERENCE so they use the new stat blobs\n",
    "    # and remove mean, var top blobs.\n",
    "    for l in test_msg.layer:\n",
    "        if l.type == \"BN\":\n",
    "            if len(l.top) > 1:\n",
    "                dead_tops = l.top[1:]\n",
    "                for dl in dead_tops:\n",
    "                    l.top.remove(dl)\n",
    "            l.bn_param.bn_mode = caffe_pb2.BNParameter.INFERENCE\n",
    "    # replace output loss, accuracy layers with a softmax\n",
    "    dead_outputs = [l for l in test_msg.layer if l.type in [\"SoftmaxWithLoss\", \"Accuracy\"]]\n",
    "    out_bottom = dead_outputs[0].bottom[0]\n",
    "    for dead in dead_outputs:\n",
    "        test_msg.layer.remove(dead)\n",
    "    test_msg.layer.add(\n",
    "        name=\"prob\", type=\"Softmax\", bottom=[out_bottom], top=['prob']\n",
    "    )\n",
    "    return net, test_msg\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_parser():\n",
    "    p = ArgumentParser()\n",
    "    p.add_argument('train_model')\n",
    "    p.add_argument('weights')\n",
    "    p.add_argument('out_dir')\n",
    "    return p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dschreib/COGS185/SegNet_Tutorial/Models/segnet_train_coral.prototxt\n"
     ]
    }
   ],
   "source": [
    "args = {}\n",
    "args['train_model'] = '/home/dschreib/COGS185/SegNet_Tutorial/Models/segnet_train_coral.prototxt'\n",
    "args['weights'] = '/home/dschreib/Corals/Training/segnet_iter_9000.caffemodel'\n",
    "args['out_dir'] = '/home/dschreib/COGS185/SegNet_Tutorial/Models/Inference/'\n",
    "\n",
    "print args['train_model']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    caffe.set_mode_gpu()\n",
    "    #p = make_parser()\n",
    "\n",
    "    # build and save testable net\n",
    "    if not os.path.exists(args['out_dir']):\n",
    "        os.makedirs(args['out_dir'])\n",
    "    print \"Building BN calc net...\"\n",
    "    testable_msg = make_testable(args['train_model'])\n",
    "    BN_calc_path = os.path.join(\n",
    "        args['out_dir'], '__for_calculating_BN_stats_' + os.path.basename(args['train_model'])\n",
    "    )\n",
    "    with open(BN_calc_path, 'w') as f:\n",
    "        f.write(text_format.MessageToString(testable_msg))\n",
    "\n",
    "    # use testable net to calculate BN layer stats\n",
    "    print \"Calculate BN stats...\"\n",
    "    train_ims, train_labs = extract_dataset(testable_msg)\n",
    "    train_size = len(train_ims)\n",
    "    minibatch_size = testable_msg.layer[0].dense_image_data_param.batch_size\n",
    "    num_iterations = train_size // minibatch_size + train_size % minibatch_size\n",
    "    in_h, in_w =(360, 480)\n",
    "    test_net, test_msg = make_test_files(BN_calc_path, args['weights'], num_iterations,\n",
    "                                         in_h, in_w)\n",
    "    \n",
    "    # save deploy prototxt\n",
    "    #print \"Saving deployment prototext file...\"\n",
    "    #test_path = os.path.join(args.out_dir, \"deploy.prototxt\")\n",
    "    #with open(test_path, 'w') as f:\n",
    "    #    f.write(text_format.MessageToString(test_msg))\n",
    "    \n",
    "    print \"Saving test net weights...\"\n",
    "    test_net.save(os.path.join(args['out_dir'], \"coral_weights_05_04.caffemodel\"))\n",
    "    print \"done\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
